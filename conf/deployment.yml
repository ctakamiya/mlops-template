# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "10.4.x-cpu-ml-scala2.12"

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "Standard_E8_v3"

environments:
  default:
    workflows:             
      - name: "porto-bank-mlops"
        job_clusters:
          - job_cluster_key: "default"
            <<: *basic-static-cluster
        tasks:
          - task_key: "etl"
            job_cluster_key: "default"
            spark_python_task:
              python_file: "file://porto_bank_mlops/tasks/sample_etl_task.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml" ]
          - task_key: "ml"
            depends_on:
              - task_key: "etl"
            job_cluster_key: "default"
            python_wheel_task:
              package_name: "porto_bank_mlops"
              entry_point: "ml"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_ml_config.yml" ]
          ###############################################################################
          # this is an example task based on the notebook                               #
          # Please note that first you'll need to add a Repo and commit notebook to it. #
          ###############################################################################
          - task_key: "notebook"
            deployment_config:
              no_package: true # we omit using package since code will be shipped directly from the Repo
            depends_on:
              - task_key: "ml"
            job_cluster_key: "default"
            notebook_task:
              notebook_path: "/Repos/claudio.takamiya@databricks/mlops_template_teste1-dev/notebooks/sample_notebook"

